{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing of Logs\n",
    "\n",
    "\n",
    "\n",
    "#### 1.- Reading and writing logs\n",
    "\n",
    "#### 2.- NLP Techniques applied to logs\n",
    "\n",
    "    2.1.- Sentence detection\n",
    "\n",
    "    2.2.- Part-Of-Speech Taging\n",
    "\n",
    "    2.3.- Named Entities Recognition\n",
    "\n",
    "    2.4.- Acronym detection\n",
    "\n",
    "    2.5.- Dependency Parser\n",
    "\n",
    "    2.6.- Lemmatization\n",
    "\n",
    "#### 3.- Creation of the new log and re-labelling of events\n",
    "#### 4.- Data Log Quality Metrics Calculation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.- Reading and writting CSV logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "LOG_FILE_PATH=\"./data/original_log.csv\"\n",
    "\n",
    "def read_csv_log(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        row = next(reader)\n",
    "        if len(row) == 3:\n",
    "            return [(inc_code, inc_type, description) for inc_code, inc_type, description in reader]\n",
    "        else:\n",
    "            return [(inc_code, description) for inc_code, description in reader]\n",
    "\n",
    "    \n",
    "def write_csv_log(log, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        for line in log:\n",
    "            writer.writerow(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of incidences:  4416\n"
     ]
    }
   ],
   "source": [
    "incidences = read_csv_log(LOG_FILE_PATH)\n",
    "\n",
    "print(\"Number of incidences: \", len(incidences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of incidences by type:\n",
      "\tTYPE \t INCIDENCES\n",
      "\t TT \t   1775\n",
      "\t NHA \t   1287\n",
      "\t HEQ \t   185\n",
      "\t PIS \t   40\n",
      "\t NHM \t   212\n",
      "\t HMC \t   33\n",
      "\t PIO \t   383\n",
      "\t HEL \t   196\n",
      "\t NHP \t   33\n",
      "\t NHT \t   272\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "type_counter = Counter([inc_type for _, inc_type, _ in incidences])\n",
    "\n",
    "print(\"Number of incidences by type:\")\n",
    "print(\"\\tTYPE \\t INCIDENCES\")\n",
    "for itype, count in type_counter.items():\n",
    "    print(\"\\t %s \\t   %d\" %(itype, count))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.- NLP Techniques applied to logs\n",
    "\n",
    "Loading NLP resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.lang.es.stop_words import STOP_WORDS\n",
    "\n",
    "nlp = spacy.load('es_core_news_md')  # Language model for Spanish\n",
    "\n",
    "stopwords = [\"warning\", \"warning:\"]  # words that occur in the logs and do not provide any information\n",
    "\n",
    "docs = {code:nlp(text) for code, _, text in incidences}\n",
    "tokens = {code: [t for t in doc] for code, doc in docs.items()}\n",
    "\n",
    "# Pre-process incidences\n",
    "raw_texts = [str(token) for _, token in tokens.items()]  # without NLP filters\n",
    "codes = [code for code, _ in tokens.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.- Sentence Detection\n",
    "\n",
    "The trained language models provided by SpaCy include all the requirements for our approach, including a simple, yet useful, sentence detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1007314995715', '1010259490543', '1014902821262', '1015282934226', '1017842527711']\n",
      "[[Se aborta para TS], [WARNING: no tenemos masa en el pin 1], [SE aborta para relanzar de nuevo], [WARNING: se comprueba la funcionalidad del X-lock cambiado siendo correcta ,asi como su indicación el la LMCP ,LMWS ,SWLP Y FWS Y ECAM .], [se aborta para configurar avion]]\n"
     ]
    }
   ],
   "source": [
    "sentences = {code:list(doc.sents) for code, doc in docs.items()}\n",
    "\n",
    "print(list(sentences.keys())[:5])\n",
    "print(list(sentences.values())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.- POS Tagging\n",
    "\n",
    "The logs are processed in order to keep just those words with a specific morphosyntactic category in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1007314995715', '1010259490543', '1014902821262', '1015282934226', '1017842527711']\n",
      "[['aborta', 'TS'], ['WARNING', 'tenemos', 'masa', 'pin'], ['aborta', 'relanzar', 'nuevo'], ['WARNING', 'comprueba', 'funcionalidad', 'X', 'lock', 'cambiado', 'correcta', 'indicación', 'LMCP', 'LMWS', 'SWLP', 'FWS', 'ECAM'], ['aborta', 'configurar', 'avion']]\n"
     ]
    }
   ],
   "source": [
    "TAGS = {\"NOUN\", \"VERB\", \"ADJ\", \"PROPN\"}    # we want to keep those words classified with these tags\n",
    "\n",
    "postags = {code:[str(token) for token in tks if token.pos_ in TAGS] for code, tks in tokens.items()}\n",
    "\n",
    "print(list(postags.keys())[:5])\n",
    "print(list(postags.values())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.- Named Entities Recognition (NER)\n",
    "\n",
    "Named entities are words or groups of words that refers to an organization, a specific person or location, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1007314995715', '1010259490543', '1014902821262', '1015282934226', '1017842527711']\n",
      "[[], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "entities = {code:[str(token) for token in tokens if token in set(docs[code].ents)] for code, tks in tokens.items()}\n",
    "\n",
    "print(list(entities.keys())[:5])\n",
    "print(list(entities.values())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4.- Acronym Detection\n",
    "\n",
    "A simple rule-based approach: a word is considered an **acronym** if it's uppercased and it does not appear in the vocabulary of the target language (in lowercases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def detect_acronyms(inc_tokens):\n",
    "    return [str(token) for token in set(inc_tokens) if str(token).isupper() and str(token).lower() not in nlp.vocab and len(str(token)) > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1007314995715', '1010259490543', '1014902821262', '1015282934226', '1017842527711']\n",
      "[[], [], [], ['SWLP', 'LMCP', 'FWS', 'LMWS'], []]\n"
     ]
    }
   ],
   "source": [
    "acronyms = {code:detect_acronyms(tks) for code, tks in tokens.items()}\n",
    "\n",
    "print(list(acronyms.keys())[:5])\n",
    "print(list(acronyms.values())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5.- Dependency Parser\n",
    "\n",
    "The words in the log are filtered out, keeping just those words with a specific function in the text (_subject, direct object, root,_ etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "DEPENDENCIES = {\"nsubj\", \"obj\"}     # Set of dependencies that we want to keep. \n",
    "                                    # We could be (a lot) more restrictive with: DEPENDENCIES = {\"ROOT\"}\n",
    "\n",
    "def dependencies(inc_tokens, dependencies=DEPENDENCIES):\n",
    "    valids = set()\n",
    "    for token in inc_tokens:\n",
    "        if token.dep_ in dependencies:\n",
    "            valids.add(str(token))              # Token with the required dependency\n",
    "            valids.add(str(token.head))         # Token that is the origin of this dependency\n",
    "    \n",
    "    return [str(token) for token in inc_tokens if str(token) in valids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1007314995715', '1010259490543', '1014902821262', '1015282934226', '1017842527711']\n",
      "[['Se', 'aborta'], ['tenemos', 'masa'], ['SE', 'aborta'], ['se', 'comprueba', 'funcionalidad', 'asi', 'indicación'], ['se', 'aborta', 'configurar', 'avion']]\n"
     ]
    }
   ],
   "source": [
    "deps = {code:dependencies(tks) for code, tks in tokens.items()}\n",
    "\n",
    "print(list(deps.keys())[:5])\n",
    "print(list(deps.values())[:5])\n",
    "\n",
    "deps_root = {code:dependencies(tks, {\"ROOT\"}) for code, tks in tokens.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6.- Lemmatization\n",
    "\n",
    "Logs where the words are changed by their lemmas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1007314995715', '1010259490543', '1014902821262', '1015282934226', '1017842527711']\n",
      "[['Se', 'abortar', 'parir', 'TS'], ['WARNING', ':', 'no', 'tener', 'masa', 'en', 'el', 'pin', '1'], ['SE', 'abortar', 'parir', 'relanzar', 'de', 'nuevo'], ['WARNING', ':', 'se', 'comprobar', 'lo', 'funcionalidad', 'del', 'X', '-', 'lock', 'cambiar', 'ser', 'correcto', ',', 'asi', 'comer', 'su', 'indicación', 'el', 'lo', 'LMCP', ',', 'LMWS', ',', 'SWLP', 'Y', 'FWS', 'Y', 'ECAM', '.'], ['se', 'abortar', 'parir', 'configurar', 'avion']]\n"
     ]
    }
   ],
   "source": [
    "lemmas = {code:[token.lemma_ for token in tks] for code, tks in tokens.items()}\n",
    "\n",
    "print(list(lemmas.keys())[:5])\n",
    "print(list(lemmas.values())[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.- Creation of the new log and re-labelling of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "connection = cx_Oracle.connect('user/pass@localhost')\n",
    "query = \"\"\"select * FROM INCIDENTS\"\"\"\n",
    "df_ora_complete = pd.read_sql(query, con=connection)\n",
    "df_ora_columns_needed = df_ora_complete.filter(items=['INCIDENCECODE','INCIDENCEDATE', 'GTICODE'])\n",
    "\n",
    "root_path = './data'\n",
    "files_list = os.listdir(root_path)\n",
    "files_list_nosent = [f for f in files_list if not 'sent' in f]\n",
    "files_list_sent = [f for f in files_list if 'sent' in f]\n",
    "save_path = './xes_files'\n",
    "\n",
    "for file_name in files_files_list_nosent:\n",
    "    complete_path_aux = os.path.join(root_path,file_name)\n",
    "    df_log_aux = pd.read_csv(complete_path_aux)\n",
    "    df_log_aux['INCIDENCECODE'] = df_log_aux['INCIDENCECODE'].apply(str)\n",
    "    df_log_aux_merged =  pd.merge(df_log_aux, df_ora_columns_needed, how='left', left_on=['INCIDENCECODE'], right_on=['INCIDENCECODE'])\n",
    "    df_log_aux_merged.to_csv(os.path.join(save_path, file_name), index=False)\n",
    "\n",
    "    \n",
    "for file_name in files_list_sent:\n",
    "    complete_path_aux = os.path.join(root_path,file_name)\n",
    "    df_log_aux = pd.read_csv(complete_path_aux)\n",
    "    df_log_aux['INCIDENCECODE'] = df_log_aux['INCIDENCECODE'].apply(str)\n",
    "    df_log_aux['INCIDENCECODE_nosent'] = df_log_aux['INCIDENCECODE']\n",
    "    df_log_aux['INCIDENCECODE_nosent'] =df_log_aux['INCIDENCECODE_nosent'].apply(lambda ic: ic.split('_')[0] if '_' in ic else ic)\n",
    "    df_log_aux_merged =  pd.merge(df_log_aux, df_ora_columns_needed, how='left', left_on=['INCIDENCECODE_nosent'], right_on=['INCIDENCECODE'])\n",
    "    df_log_aux_merged_to_save = df_log_aux_merged.filter(items=['INCIDENCECODE_x','INCIDENCEDATE', 'GTICODE', 'DESCRIPTION'])\n",
    "    df_log_aux_merged_to_save.to_csv(os.path.join(save_path, file_name), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.- Data Log Quality Metrics Calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cx_Oracle\n",
    "import collections\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import pprint\n",
    "import seaborn as sns\n",
    "import xmltodict\n",
    "from json import dumps,loads\n",
    "import xml.etree.ElementTree as xml\n",
    "from xml.etree import ElementTree\n",
    "from xml.etree.ElementTree import Element, SubElement\n",
    "from xmlr import xmlparse\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pp = pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_consistency(new_traces_2, avg_length_events_string, total_events):\n",
    "    consistency_result = 0\n",
    "    for elem in new_traces_2:\n",
    "        aux_events_list = elem['events']\n",
    "        for e_i in aux_events_list:\n",
    "            e_i_value = abs(len(e_i)-avg_length_events_string)/total_events\n",
    "            consistency_result += e_i_value\n",
    "            \n",
    "    return consistency_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(path):\n",
    "    xml_string_xes = open(path,mode='r', encoding='utf-8').read()\n",
    "    log_xes = xmltodict.parse(xml_string_xes)\n",
    "    log_xes = loads(dumps(log_xes))\n",
    "    traces = log_xes['log']['trace']\n",
    "    new_traces = list(map(lambda x: {'id_trace': x['string']['@value'], 'events': x['event']}, traces))\n",
    "    \n",
    "    new_traces_2 = []\n",
    "    events_set=set()\n",
    "    different_per_trace= dict()\n",
    "      \n",
    "    for i in new_traces:\n",
    "        incidencecodes = []\n",
    "        new_events = []\n",
    "        for e in i['events']:\n",
    "            #Use 'int' if INCIDENCECODE are int or 'string' if they are string in the XES log.\n",
    "#             incidencecode = [d['@value'] for d in e['int'] if d['@key']=='INCIDENCECODE'][0]\n",
    "            incidencecode = [d['@value'] for d in e['string'] if d['@key']=='INCIDENCECODE'][0]\n",
    "            if incidencecode not in incidencecodes:\n",
    "                incidencecodes.append(incidencecode)\n",
    "                new_events.append([d['@value'] for d in e['string'] if d['@key']== 'concept:name'][0])\n",
    "    \n",
    "        new_traces_2.append({'id_trace':i['id_trace'], 'events':new_events})\n",
    "        events_set.update(new_events)\n",
    "        different_per_trace[i['id_trace']] = len(set(new_events))\n",
    "    \n",
    "    avg_length_events_string = sum( map(len, list(events_set)) ) / len(list(events_set))\n",
    "    traces_count_events = list(map(lambda x: {x['id_trace']: len(x['events'])}, new_traces_2))\n",
    "    \n",
    "    traces_count_events_dict = dict()\n",
    "    for d in traces_count_events:\n",
    "        for k,v in d.items():\n",
    "            traces_count_events_dict[k] = v\n",
    "    \n",
    "    total_events = sum([list(d.values())[0] for d in traces_count_events])\n",
    "    \n",
    "    events_dict = dict()\n",
    "    for i in new_traces_2:\n",
    "        for e in i['events']:\n",
    "            if e in events_dict.keys():\n",
    "                events_dict[e] = events_dict[e] + 1\n",
    "            else:\n",
    "                events_dict[e] = 1\n",
    "\n",
    "    lonely_events = [k for k, v in events_dict.items() if v == 1]\n",
    "    \n",
    "    lonely_events_per_trace = dict()\n",
    "\n",
    "    for d in new_traces_2:\n",
    "        lonely_events_per_trace[d['id_trace']] = len([e for e in d['events'] if e in lonely_events])\n",
    "        \n",
    "    consistency = compute_consistency(new_traces_2, avg_length_events_string, total_events)\n",
    "    \n",
    "    \n",
    "    return traces_count_events_dict, lonely_events_per_trace, total_events, len(list(events_set)), len(lonely_events), len(traces_count_events_dict), total_events/len(traces_count_events_dict), different_per_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_lonely_events_events(lonely_events_per_trace, events_per_trace):\n",
    "    average_lonely_events_dict = dict()\n",
    "    for k, v in lonely_events_per_trace.items():\n",
    "        average_lonely_events_dict[k] = (v/events_per_trace[k])\n",
    "    \n",
    "    return average_lonely_events_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_roughest_trace(average_lonely_events_per_trace):\n",
    "    df = pd.DataFrame()\n",
    "    df['id_trace'] = list(average_lonely_events_per_trace.keys())\n",
    "    df['average'] = list(average_lonely_events_per_trace.values())\n",
    "    average_mean = df['average'].mean()\n",
    "    df['sd'] = df.apply(lambda x: np.std([x['average'], average_mean]), axis=1)\n",
    "    return max(list(df['sd']), default=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_noise_in_log(total_events, total_lonely_events):\n",
    "    return total_lonely_events/total_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diversity_in_log(total_events, total_different_events):\n",
    "    return total_different_events/total_events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diversity_average(different_events_per_trace, events_per_trace):\n",
    "    diversity_events_dict = dict()\n",
    "    for k, v in different_events_per_trace.items():\n",
    "        diversity_events_dict[k] = (v/events_per_trace[k])\n",
    "    \n",
    "    return diversity_events_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_diversity_disparate_trace(average_diversity_per_trace):\n",
    "    df = pd.DataFrame()\n",
    "    df['id_trace'] = list(average_diversity_per_trace.keys())\n",
    "    df['average'] = list(average_diversity_per_trace.values())\n",
    "    average_mean = df['average'].mean()\n",
    "    df['sd'] = df.apply(lambda x: np.std([x['average'], average_mean]), axis=1)\n",
    "    return max(list(df['sd']), default=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_composed_metrics(total_events, total_lonely_events, total_different_events, events_per_trace, lonely_per_trace, different_per_trace):\n",
    "    noise_in_log = get_noise_in_log(total_events, total_lonely_events)\n",
    "    average_lonely_per_trace = get_average_lonely_events_events(lonely_per_trace, events_per_trace)\n",
    "    max_noise = get_noise_roughest_trace(average_lonely_per_trace)\n",
    "    diversity_in_log = get_diversity_in_log(total_events, total_different_events)\n",
    "    average_diversity_per_trace = get_diversity_average(different_per_trace, events_per_trace)\n",
    "    max_diversity = get_diversity_disparate_trace(average_diversity_per_trace)\n",
    "    \n",
    "    return noise_in_log, max_noise, diversity_in_log, max_diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(path):\n",
    "    res = get_metrics(path)\n",
    "    res_composed = get_composed_metrics(res[2], res[4], res[3], res[0], res[1], res[7])\n",
    "    print('EVENTS: ', res[2])\n",
    "    print('DIFFERENT EVENTS: ', res[3])\n",
    "    print('LONELY EVENTS: ', res[4])\n",
    "    print('TRACES: ', res[5])\n",
    "    print('COMPLEXITY: ', res[6])\n",
    "    print('UNIQUENESS: ', res_composed[0])\n",
    "    print('RELEVANCY: ', res_composed[2])\n",
    "    print('CONSISTENCY: ', res[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example of metrics computation\n",
    "print_metrics('./data/log_sentences_dep_lemmas_root.xes') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
